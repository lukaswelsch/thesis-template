\chapter{Daten}\label{ch:data}
Die in der Arbeit verwendeten Daten stammen aus dem Data Warehouse einer deutschen Bank.
Es wird mit den Entwicklungsdaten gearbeitet, da diese im wesentlichen den produktiven Daten entsprechen.
In den Entwicklungsdaten wurden personenbezogene Daten, aufgrund des Datenschutzes, bereits pseudonymisiert und anonymisiert.
Da der Finanzdienstleister von dem die Daten stammen sich hauptsächlich auf Kredite für Privatkunden spezialisiert hat, werden die Daten der Privatkunden analysiert.

Um den exakten Nutzen und das Problem des Unternehmens zu lösen ist es notwendig eine Analyse der Geschäftsprozesse und der Stakeholder durchzuführen. 
Diese wird in \autoref{sec:Stakeholder-Analyse} ausgeführt und die Anforderungen der einzelnen Teilnehmer erläutert und begründet.
Insgesamt geht aus der Stakeholderanalyse hervor, dass die Datenqualität auf verschiedenen Ebenen analysiert werden sollen und müssen. 

In dieser Bachelorarbeit wird die Datenqualität auf beispielhaft auf drei Arten untersucht:
\begin{enumerate}
 \item \textbf{Texte} \\
 Zum einen werden textuelle Daten untersucht, die von Nutzern händisch angelegt wurden und somit Rechtschreibfehler oder unbekannte Abkürzungen enthalten könnten. 
 \item \textbf{Besondere Attribute} \\
 Als weiterer Fall werden sensible Attribute betrachtet, deren Korrektheit von besonderer Bedeutung ist. Das Attribut muss von anderen Attributen abhängig sein, um mit Hilfe eines Klassifikators analysiert und verbessert zu werden.  %Das einen hohen Stellenwert hat? und von anderen Attributen abhängen 
 \item \textbf{Datenextraktion} \\
 Des weiteren werden die Datenextraktionen untersucht. Für diese wird eine Möglichkeit dargestellt, wie mit Hilfe einer Visualisierung, geprüft werden kann, ob Fehler in den Daten durch Programm- oder Infrastrukturänderungen entstanden sind. 
\end{enumerate}



Ein- und Ausschlusskriterien für die Daten:\\
In diesem Projekt werden verschiedene Datensätze verwendet. 
Es werden textuelle Daten benötigt, die durch Nutzer angelegt werden und deshalb Rechtschreib-, Grammatik oder ähnliche Fehler enthalten. 
Hierfür wird eine Möglichkeit geschaffen diese Fehler messbar zu machen und darzustellen. \\ 

Des weiteren werden Daten benötigt, die für Machine Learning geeignet sind. 
Hierfür werden Daten benutzt, die feste Labels zu bestimmten Input Parametern besitzt.
Ein Klassifikator kann anhand der Eigenschaften einer Ausprägung und dem dazugehörigem Label lernen, welche Eigenschaften zu welchem Label führen und so eine Klassifikation durchführen. \\

Des weiteren wird ein Datensatz benötigt, der für die Visualisierung verwendet werden kann. 
Dieser Datensatz benötigt Eigenschaften, die sich so visualisieren lassen, dass anhand der Visualisierung neue Erkenntnisse abgeleitet werden können.
Am besten bieten sich Daten an, die einen Zeitstempel besitzen, da sich so ein temporaler Zusammenhang darstellen lässt. \cite{pastorello2014}
\\





Zur Überprüfung der Rechtschreibung der Daten sind insbesondere Fehler in den Berufen der Kunden festzustellen. 
Diese werden durch ein System von einem Bankmitarbeiter abgelegt. 
Hierbei kommt es zu Rechtschreibfehlern durch die Mitarbeiter. 
Es ist für ein Computersystem schwierig diese Daten später zu interpretieren.
Insbesondere ist dies schwierig für die unterschiedlichen Abkürzungen, die verwendet werden. 
Diese Abkürzungen entsprechen keinem Standard und es ist nicht möglich diese automatisch auf die korrekte Form zurückzuführen. 
Des Weiteren sind die Felder nicht vollständig, da einige der Berufsfelder gar nicht oder nur mit einem Bindestrich befüllt sind. 
Die Daten sind in hoher Stückzahl vorhanden und für die Stakeholder wichtig. 
Diese Daten bieten sich deshalb gut an, um sie für die Analyse zu verwenden. 


Nach einer Recherche und Analyse der vorhandenen Daten bieten sich die Daten zum Risikoscoring für das Vorhersagen an.
Diese Daten sind in einer Vielzahl vorhanden und erfüllen die gewünschten Anforderung an Machine Learning. 
Die Daten sind nachfolgend genauer beschrieben: \\
Das Label ist der Risikowert eines Geschäfts, wobei ein Geschäft in diesem Fall ein Darlehen ist.
Dieser Risikowert hat einen Wertebereich von 0E bis 4E.
Die Inputdaten bestehen aus den folgenden Eigenschaften, die zum Training des Klassifikators verwendet werden können. 


%TODO Für die Visualisierung können Metadaten zu den Prozessen verwendet werden.
%Diese beinhalten die Anzahl der verarbeiteten Datensätze pro Zeiteinheit.





% Ein Beispiel für Daten, die aggregiert werden 

Nach \cite{petri2005} verwenden Auskunfteien bestimmte Daten als Grundlage des Risikoscores. 
Darunter werden Kfz-Besitz, Wohndauer, verfügbares Einkommen, Berufsdaten und Haftende nicht im Data Warehouse gespeichert. 
Somit ergibt sich folgende Liste relevanter Daten für die ML-Experimente. 

\begin{itemize}
 \item Alter (Geburtsdatum)
 \item ausgeübter Beruf
 \item Bürge vorhanden
 \item Familienstand
 \item Geschlecht
 \item Kinderanzahl
 \item Kredite in Stück 
 \item Nettokreditbetrag 
 \item Nationalität
 \item Schufaauskunft
 \item Haushaltstyp
\end{itemize}


Im Rahmen der Recherche zu Daten, die sich zur Berechnung des Risikoscores anbieten konnten weitere Daten identifiziert werden, die einen Mehrwert zur Klassifikation bringen könnten. 
Diese sind der aktuelle Rückstand der Kreditzahlung und Daten aus öffentlichen Schuldnerverzeichnissen.
In dem Kapitel Experimente werden die ML-Algorithmen einmal mit und einmal ohne diesen zusätzlichen Eigenschaften getestet, um festzustellen, ob sich somit ein besseres Ergebnis erzielen lässt. 
% Wertebereich der Daten, Anzahl der Daten


\section{Extraktion der Daten / Erstellen der ABT}
Zunächst wird für die Daten ein neues Datenbank-Schema angelegt.
Ein Schema ist ein Ordner für Tabellen innerhalb einer Datenbank.

Im ersten Schritt werden die Tabellen mit Hilfe eines 'CREATE TABLE' erstellt, die gleich mit den Quelltabellen sind. 
Diese umfassen beispielsweise die Tabellen Rating, Kundenstammdaten und Geschäftsstammdaten. 
Anschließend werden diese Tabellen befüllt, indem die Daten von einem anderem Server extrahiert werden, auf dem die Daten bereits anonymisiert und pseudonymisiert gespeichert sind. 
Diese Daten entsprechen den Originaldaten, die zum Testen neuer SQL-Skripte verwendet werden. 
Da bei diesem DataWarehouse eine DataVault Architektur eingesetzt wird, müssen zu den Satelliten, die die Daten enthalten auch die Verknüpfungstabellen, Links genannt, extrahiert werden.  

Im zweiten Schritt werden die Daten mit Hilfe eines ETL-Prozesses aus dem DataWarehouse extrahiert. 
Die Extraktion erfolgt mit Hilfe eines proprietären Tools: Ab Initio. 
Dieses bietet eine grafische Entwicklungsumgebung, eine Versionsverwaltung und eine Ablaufplanung für Programme. 
Der Vorteil dieses Programms liegt darin, dass die Anbindung an die Datenbank vorkonfiguriert ist, durch das visuelle Programmieren die Anzahl der Daten direkt gesehen werden kann und somit weniger Fehler durch die Extraktion zu sehen sind. 
Des weiteren ist Ab Initio schneller als die Datenbank, da der Server, der das Programm ausführt mehr Leistung besitzt und besser optimiert ist für die Datenextraktion. 
Siehe für genauere Erläuterungen folgende Webseite: \cite{https://www.abinitio.com/de/system/the-cooperating-system}
Dieses Programm wurde im Rahmen der vorliegenden Arbeit entwickelt und ist in einer Versionsverwaltung gespeichert. 

%Das Ergebnis der Extraktion ist eine Analytical Base Table (ABT), die nach den  vorher genannten Kriterien (sokol) ausgewählt und erzeugt wird


%Programmiersprache, ETL Programm
Zunächst werden die benötigten Daten aus insgesamt zwölf Quelltabellen %Namen der Tabellen
geladen, hierzu zählen z.B. personenbezogene Daten oder auch die tatsächlichen Risikoscorings. 
Anschließend werden die Daten bereinigt und mit Hilfe einer Aggregatfunktion die Kredite bzw. Saldo summiert und die Anzahl in einer zusätzlichen Spalte gespeichert.
Diese MetaInformationen werden mitgespeichert, da diese einen positiven Einfluss auf die Güte des Klassifikators haben könnte. 
Dies wird in den Experimenten getestet, indem der Klassifikator mit und ohne diese Zusatzinformationen trainiert wird. 
Die Daten zu den Kindern, die die Kunden haben, werden in einer extra Tabelle gespeichert.
Es wird beispielsweise nicht die Anzahl der Kinder gespeichert sondern nur der Eintrag zu jedem einzelnem Kind. 
Mit Hilfe einer Aggregation auf die ID des Kunden wird die Anzahl der Kinder berechnet und gespeichert. 



\section{Informationen zu den Daten / Aufbau der Daten}



\section{Eigenschaften der Daten}
Insgesamt wurden knapp sieben Million Datensätze extrahiert. 
Da zunächst die tagesaktuellen Daten verwendet werden, stehen allerdings nur 150 Tausend Datensätze für die ML-Experimente zur Verfügung. 
Für die Visualisierung und das Dashboard werden alle Datensätze verwendet, die extrahiert werden. 
Einzelne Eigenschaften und ihre Wertebereich sind in der nachfolgenden Tabelle aufgeführt. 

\begin{tabular}[h]{l|l|l}
Datenwert & Anzahl unterschiedlicher Werte  & Wertebereich \\ \hline
KundenID & 150.000  &  \\ \hline
PLZ & 6500  & 1015; 99974 \\ \hline
Geburtsjahr & 90  & 1913; 2001 \\ 

\end{tabular}

In der Tabelle ist eine ungewöhnliche Postleitzahl zu lesen 1015. 
Dies kommt zustande, da in den Daten nicht nur Deutsche, sondern auch internationale Adressen abgespeichert sind. 
Des weiteren sind die Berufe nicht genderneutral abgespeichert, sondern beinhalten die Formen für männliche / weibliche Bezeichnungen, wie zum Beispiel: Auszubildende. 
Im \autoref{ch:method} wird hierfür die Methode des Stemmings genannt und erläutert.
Mit diesem Verfahren ist es möglich die Berufe auf einen Wortstamm zurückzuführen.


\textbf{Verteilung der Klassen/Klassendichte}
Um genauere Aussagen über die Daten treffen zu können, wird nachfolgend die Verteilung der Klassen berechnet und dargestellt. 
Diese stellen die Grundlage für die Entscheidung der Sampling-Verfahren, die im Kapitel 4 Methoden beschrieben sind, dar.
Mit Hilfe der in DB2 verfügbaren Analytischen Funktion 'ratioToReport' kann nachfolgendes SQL geschrieben werden, dass die prozentuale Verteilung der Klassen darstellt. 

\begin{verbatim}
SELECT ratioToReport(count(count(*)) over() * 100) as ANTEIL 
FROM BACHELORDATEN 
GROUP BY RATINGWERT
\end{verbatim}


Anhand der nachfolgenden Tabelle ist zu erkennen, dass die Klassen sehr ungleichmäßig verteilt sind. 
Die Klasse 0E ist mit 58 \% deutlich häufiger verbreitet, als die anderen Klassen. 
Auch ist zu erkennen, dass die logisch folgende Klasse 4C einen Anteil von 0\% enthält. 
Die schlechte Verteilung der Klassen ist damit zu erklären, dass bevorzugt Kunden einen Kredit bekommen, die auch eine gute Risikobewertung haben. 
Kunden, die eine schlechte Bewertung haben, werden häufig gar nicht in die engere Auswahl genommen und tauchen somit auch nicht als Kunden im Data Warehouse auf. 
Allerdings sind Kunden, die erst nach einiger Zeit kreditunwürdig werden bzw. ihren Zahlungen nicht Folge leisten können, im Data Warehouse weiterhin gespeichert. 

\begin{tabular}[h]{l|l}
Ratingwert & Anteil (in Prozent)   \\ \hline
0E & 58\% \\ \hline
1A & 8\% \\ \hline
1B & 8\% \\ \hline
1C & 5\% \\ \hline
1D & 4.6\% \\ \hline
1E & 4\% \\ \hline
2A & 2\% \\ \hline
2B & 2\% \\ \hline
2C & 2\% \\ \hline
2D & 1\% \\ \hline
2E & 1\% \\ \hline
3A & 0.7\% \\ \hline
3B & 0.5\% \\ \hline
3C & 0.3\% \\ \hline
3D & 0.1\% \\ \hline
3E & 0.03\% \\ \hline
4A & 0.004\% \\ \hline
4B & 0.04\% \\ \hline
4C & 0\% \\ \hline
4D & 0.4\% \\ \hline
4E & 0.02\% \\ 

\end{tabular}

% Anzahl der Datensätze
% Datum von bis?
% evtl. Anzahl der Risikowerte
% Anzahl der Distinct Values



\section{Daten für die Visualisierung}
Für die Visualisierung können die gleichen Daten verwendet werden, die für das ML-Modell verwendet werden.
Der Unterschied besteht jedoch darin, dass für das ML-Modell die aktuellsten Daten verwendet und für die Visualisierung die Historie der letzten drei Monate verwendet wird. 
Dies ist notwendig, da in der Visualisierung ein historischer Kontext und die Zeitreihen visualisiert werden. 



% TODO Noch nötig? 
% TODO sollte beschrieben werden. Daten können nicht verwendet werden, da zu unterschiedlich und dadurch wenig aussagekräftig (Aufgrund von umstellungen auf Delta, wieder komplettladungen etc) 
%Des weiteren werden auch Daten verwendet, die generelle Informationen zu den Prozessen im DataWarehouse enthalten. 
%Diese Daten umfassen die Anzahl der exportierten Datensätze aus den einzelnen Umgebungen.
%Außerdem sind Metadaten zu den Prozessen vorhanden.
%Diese geben Auskunft über die Ausführungszeit der einzelnen Prozesse, dem freien Arbeitsspeicher  und der CPU-Auslastung.




