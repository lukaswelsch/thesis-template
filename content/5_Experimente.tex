\chapter{Experimente}\label{ch:experiments}
%Auf Validität und Reliabilität achten!


- Aufbau des ETL-Prozesses - 

            ML     ML
-> BASE -> SRC -> BIZ

\section{ML-Experimente (evtl in genaues Verfahren umbennen)}
Scoringverfahren:

Vollständigkeit
- Fehlen Scoring ergebnisse können diese nachberechnet werden

Akutalität
- Sind die Scoring werte veraltet so können diese neu berechnet werden


Richtigkeit
- Überprüfung der Richtigkeit schwierig, da mit Daten gearbeitet wird, die evtl. selbst nicht richtig sind. 

Was sind Daten die Einfluss auf das Kundenrating haben?
-> Für welche Daten wäre es besonders wichtig die Datenqualität händisch zu überprüfen


ML-Verfahren, immer inklusive Performance-Grafik:
- logistische Regression


Ziele:
- Vorhersagen des Risikoscorings anhand der Inputdaten

Aufbau:
- python als Programmiersprache
- Aufteilen in train-dev-test
- 


\subsection{KNN}

Variabilität (Einfluss der Parameter auf das Ergebnis):
- verschiedene k-Werte 
- Manhattan vs. Euklidische Distanz


\subsection{SVM (Multiklassen)}

Verschiedene Parameter:
- unterschiedl. Kernelfunktionen
- C-Parameter, der über Slackvariablen gesteuert wird
- 

- Neuronale Netze

- Classifikation Tree




-

Aufbau Experimente: 
Ziele*ʹ Aufbau*ʹ Ergebnisse*' Interpretation*ʹ Threats*to*Validity
(Seite 93 https://userpages.uni-koblenz.de/~laemmel/esecourse/slides/perf.pdf)


Ideen:
- Komplexe Funktionen mit Stakeholdern basteln, zb wenn verheiratet dann Alter > 18
- Daten für zb Aktualität müssen definiert werden, ob sie beispielsweise überhaupt verfallen können. Zb Geburtsdatum ändert sich nie; Alter schon
- Ist es möglich solche Regeln mit HIlfe von ML abzuleiten oder funktioniert das gar nicht? 
- Daten vor einem Monat berechnen, wie viele sich ändern müssten (aufgrund von zb Timeliness, correctness) und dann nachschauen wie viele sich tatsächlich geändert haben
- Big Data Quality A Quality Dimension evaluation hat zwei konkrete Experimente, dort kann man sich gute Ideen holen. Es wird auch ein Experte zu Rate gezogen, der beispielsweise angibt, welche Daten gelöscht werden können (zb wenn 80\% der Attribute fehlen). Es hat auch einige Visualisierungen 
- Mit SQL: 
https://dataform.co/blog/advanced-data-quality-testing




Auf die verschiedenen Ebenen Aktualität, Richtigkeit, Vollständigkeit und Konsistenz eingehen!
Oft ist es besser die Daten nachzufordern, anhand eines möglichen Fehlers kann nicht der Originalzustand wiederhergestellt werden

\section{Visualisierung}

%https://www.elastic.co/guide/en/kibana/6.8/createvis.html

Integration der Daten in Kibana
- ETL Prozess zur automatischen Erzeugung einer CSV-Datei.
- Logstash?
- 

Kibana, Graphana

Reduktion der Datenmenge



- Welche Visualisierungen bieten sich an?
- Gibt es evtl. Visualisierungen, die DQ-Probleme aufzeigen?


