\chapter{Experimente}\label{ch:experiments}
%Auf Validität und Reliabilität achten!


- Aufbau des ETL-Prozesses - 

            ML     ML
-> BASE -> SRC -> BIZ

\section{ML-Experimente (evtl in genaues Verfahren umbennen)}
Scoringverfahren:

Vollständigkeit
- Fehlen Scoring ergebnisse können diese nachberechnet werden

Akutalität
- Sind die Scoring werte veraltet so können diese neu berechnet werden


Richtigkeit
- Überprüfung der Richtigkeit schwierig, da mit Daten gearbeitet wird, die evtl. selbst nicht richtig sind. 

Was sind Daten die Einfluss auf das Kundenrating haben?
-> Für welche Daten wäre es besonders wichtig die Datenqualität händisch zu überprüfen





- In den Datensatz werden die typischen Fehler eingebaut:

Vollständigkeit
- Felder sind null 
- Zeilen fehlen

Richtigkeit
- Daten haben Fehler zb. Umsatz ist zu groß
- Prozentzahl des Zinssatzes sind falsch
- Datum ist invalide

Aktualität
- Daten sind zu Alt


Konsistenz
-> Hinzufügen von einigen fiktiven Daten 
- Daten werden summiert und anhand der Summen wird erkannt, ob diese sehr abweichen




-

Aufbau Experimente: 
Ziele*ʹ Aufbau*ʹ Ergebnisse*' Interpretation*ʹ Threats*to*Validity
(Seite 93 https://userpages.uni-koblenz.de/~laemmel/esecourse/slides/perf.pdf)


Ideen:
- Komplexe Funktionen mit Stakeholdern basteln, zb wenn verheiratet dann Alter > 18
- Daten für zb Aktualität müssen definiert werden, ob sie beispielsweise überhaupt verfallen können. Zb Geburtsdatum ändert sich nie; Alter schon
- Ist es möglich solche Regeln mit HIlfe von ML abzuleiten oder funktioniert das gar nicht? 
- Daten vor einem Monat berechnen, wie viele sich ändern müssten (aufgrund von zb Timeliness, correctness) und dann nachschauen wie viele sich tatsächlich geändert haben
- Big Data Quality A Quality Dimension evaluation hat zwei konkrete Experimente, dort kann man sich gute Ideen holen. Es wird auch ein Experte zu Rate gezogen, der beispielsweise angibt, welche Daten gelöscht werden können (zb wenn 80\% der Attribute fehlen). Es hat auch einige Visualisierungen 
- Mit SQL: 
https://dataform.co/blog/advanced-data-quality-testing


\section{Fragebogen}

- Fragebogen an die Stakeholder (Es besteht die Möglichkeit durch einen Fragebogen die vorherrschende Datenqualität mit Hilfe der Stakeholder zu berechnen. \cite{pipino2002})

Auf die verschiedenen Ebenen Aktualität, Richtigkeit, Vollständigkeit und Konsistenz eingehen!
Oft ist es besser die Daten nachzufordern, anhand eines möglichen Fehlers kann nicht der Originalzustand wiederhergestellt werden

\section{Visualisierung}
Kibana, Graphana

Reduktion der Datenmenge



- Welche Visualisierungen bieten sich an?
- Gibt es evtl Visualisierungen, die DQ-Probleme aufzeigen?


