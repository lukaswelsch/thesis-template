\chapter{Methoden}\label{ch:method}
Um die nachfolgenden Methoden zur Analyse und Visualisierung von Datenqualität, zu vergleichen werden sowohl qualitative als auch quantitative Untersuchungen durchgeführt.
Für eine Datenqualität, die sich in einem Unternehmen etablieren kann, ist es nötig diese von den Gesichtspunkten aller Stakeholdern zu betrachten.
Anhand einer Stakeholderanalyse wird es möglich sein die Probleme der Datenqualität auf zwei Ebenen zu betrachten. 
Zum einen die der Business-User, die gute Datenqualität benötigen, um Kampagnen (Werbung) gezielt an die richtigen Kunden zu senden.
Zum anderen benötigen die Entwickler eine Validierung (Überprüfungsmöglichkeit) für die ETL-Prozesse, die sie entwickelt haben, um sicherzustellen, dass diese keine (neuen) Datenqualitätsprobleme erzeugen. 
% (In Worten zusammengefasst und durch Statistiken und Schaubilder) 


\section{Stakeholderanalyse}
Mit Hilfe einer Stakeholderanalyse ist es möglich, die negativen Einflüsse zu erkennen und die positiven Einflüsse zu nutzen.
Zum Anderen können die Erwartungen der einzelnen Stakeholder erkannt werden und die Projektziele richtig gewichtet werden. 

Es besteht die Möglichkeit durch einen Fragebogen die vorherrschende Datenqualität mit Hilfe der Stakeholder zu berechnen. \cite{pipino2002}
Dies fällt unter den subjektive Messbarkeit der Datenqualität und \textit{wird im Kapitel 5.Experimente durchgeführt und dargestellt.}

1. Identifikation
2. Information und Analyse
3. Aktionsplanung
4. Monitoring


- Der Benutzer von Daten
- Der Entwickler
- Der, der zur Einhaltung der korrekten Datenqualität da ist


Nach \cite{pipino2002} gibt es drei Stakeholder: the collector, custodians and consumers of data products.  

Aufgrund der Analyse des mittelständischen Unternehmens in der Bankenbranche können folgende Stakeholder identifiziert werden: \\
Als Stakeholder lassen sich zunächst die Entwickler identifizieren.
Diese sind für die Einhaltung einer guten Datenqualität unmittelbar wichtig, da diese die Extraktionsprozesse entwickeln, die zu Fehlern in der Datenqualität führen können.
Auf der anderen Seite gibt es die Business User, die Kampagnen für die Banken entwickeln, um beispielsweise neue Kunden zu gewinnen. 
Auch für diese Gruppe von Personen ist es wichtig, dass die Daten fehlerfrei sind, da sonst (evtl.) Kunden angesprochen werden, die gar nicht relevant für die Kampagne sind.
Eine weitere Rolle ist die des Datenmanagers, der überprüft, ob die Datenqualität gut genug ist und diese im Überblick behält und gegebenenfalls Maßnahmen zur Besserung einleitet. 
Des weiteren ist der Kunde ein Bestandteil einer datenverarbeitenden Abteilung. 
Dieser erwartet nicht nur, dass die Daten fehlerfrei sind, sondern auch, dass diese immer verfügbar sind. 
Der Aspekt der Verfügbarkeit der Daten hat nur bedingt etwas mit der Datenqualität zu tun und wird deshalb nicht weiter in dieser Arbeit betrachtet.
Für ein umfassendes Projekt, bei dem alle Stakeholder zufriedengestellt werden, müsste dieser Aspekt in die Konzeption einfließen.

Die beschriebenen Stakeholder werden mit ihren Erwartungen an das Projekt in der folgenden Grafik dargestellt.

\begin{tabular}[h]{l|p{2cm}|>{\centering}p{1.5cm}|p{2.5cm}|c|c|p{3cm}}
ID & Wer        & Betroffen-heit & Erwartung & Macht & Einstellung & Maßnahmen  \\ \hline
S1 & Entwickler & m             & Fehler in der Entwicklung werden angezeigt; Wenig Programmieraufwand & g & Neutral & Erklärung der Notwendigkeit, Zeitvorteil aufzeigen  \\ \hline
S2 & Business User & h          & Daten sollten fehlerfrei sein & g & Positiv & - \\ \hline
S3 & Daten-manager & h          & Überprüfung der Qualität muss jederzeit und einfach möglich sein & g & Positiv & - \\ \hline
S4 & (Bank)-Kunde & h          & Daten müssen immer verfügbar und richtig sein & h & Positiv & - \\
\end{tabular}


Welchen Stakeholder interessiert welche Dimension?


Die Entwickler benötigen einen Mechanismus oder ein Programm, dass ihnen zeigt, ob durch eine Neuentwicklung Fehler in den Datenentstehen, hier ist besonders die Datenqualitätsdimension Vollständigkeit wichtig. 


\begin{tabular}[h]{l|l|l}
Stakeholder & Dimension & Begründung \\ \hline
Entwickler & Vollständigkeit & (Wurden alle Daten vom Quellsystem abgeholt) \\ \hline
Business User & Alle Dimensionen & \\ \hline
Datenmanager & & \\ \hline
Kunde & Keine Dimension & Geht davon aus, dass die Datenqualität schon geprüft wurde \\
\end{tabular}



\section{Metriken}
Nach \cite{pipino2002} besteht die Schwierigkeit nicht darin die Metriken zu formulieren, sondern die Datenqualitätsdimension zu definieren, die auf den spezifischen Anwendungsbereich des Unternehmens passt. 
Aus BI-Sicht ist es enorm wichtig, dass die Daten korrekt sind. 
Beispielsweise würde es unnötige Kosten verursachen einem Kunden eine Werbung zu zusenden, wenn dieser schon umgezogen ist.
Für diese Fehler kann als Indiz die Aktualität verwendet werden und Kunden können nicht angeschrieben werden, wenn die Wahrscheinlichkeit groß ist, dass diese schon umgezogen sind. 
Nicht nur die Daten selbst müssen den Datenqualitätsanforderungen stimmen, sondern es muss auch sichergestellt werden, dass die ETL-Prozesse fehlerfrei ablaufen. 
Sonst würden durch die Extraktionen und Anreicherungen neue Datenqualitätsfehler in den Daten entstehen.


\textbf{Simple Ratio:}
Die Simple Ratio kann für die Dimensionen Richtigkeit, Vollständigkeit und Konsistenz verwendet werden und ist wie folgt aufgebaut: \cite{pipino2002}
$$ 1 - \frac{erwartete Anzahl}{Gesamtzahl} $$
Das Problem an der Simple Ratio liegt darin, dass nicht unbedingt bekannt ist, wie viele Daten zu erwarten sind. 
Würden nur die null-Werte gezählt werden ist nicht klar, ob die Daten tatsächlich fehlen, unbekannt sind oder nicht existent.
Bei einer Telefonnummer könnte null bedeuten, dass diese nicht bekannt ist, dass derjenige kein Telefon und somit keine Telefonnummer besitzt oder, dass nicht bekannt ist, ob es derjenige eine Telefonnummer besitzt. 
Diese Metrik kann gut für die Überprüfung der Prozessqualität verwendet werden, da in diesem Fall die erwartete Anzahl und die Gesamtzahl bekannt sind.


Desired outcomes to total outcomes 
-> geeignet für Richtigkeit, Vollständgkeit, Konsistenz


Maximum operation
-> geeignet für Aktualität

Weighted Average. 

\textbf{Wahrscheinlichkeitsverteilung zur Schätzung der Aktualität}
Die Dimension Aktualität ist besonders Interessant für die Datenqualität, da diese als Wahrscheinlichkeit angegeben werden kann.
Hierfür gibt es verschiedene Ansätze, der aktuellste liegt darin eine Wahrscheinlichkeits- bzw. Dichtefunktion zu schätzen und anhand dieser konkrete Wahrscheinlichkeiten zu berechnen, ob die Daten schon veraltet sind.
Um diese Dichte zu schätzen können externe Daten verwendet werden (wie oft ziehen Menschen um, wie viele Eheschließung gibt, wie oft lassen sich Paare scheiden).
Da in diesem Fall auf einen vollständig historisierten Datensatz zurückgegriffen werden kann, kann die Dichtefunktion anhand der durchschnittlichen Lebensdauer der Attribute berechnet werden.
Dabei ist sehr kritisch zu betrachten, dass die Daten aus denen die Funktion geschätzt wird, selbst Fehler enthalten können. 

Aktualität:
- Externe Daten verwenden zb. Anzahl der Eheschließungen / Scheidungen) 
- historische Daten aus dem Data Warehouse, wie lange ist im Schnitt eine Adresse gültig
- Wie lange ist im Schnitt ein Attribut gültig
--> Achtung! Daten, die zur Berechnung genommen werden können selbst von schlechter DQ sein
--> Das ist kritisch zu betrachten


Stichproben für Schätzung


Vollständigkeit: Metrik entwickeln!



\section{Auf Datenebene}


Richtigkeit
Um die Richtigkeit zu berechnen gibt es zwei verschiedene Möglichkeiten. 

\subsection{Machine Learning}
- Unüberwachtes Lernen findet mögliche Fehlerquellen
- Fehlerquellen werden identifiziert
- Mit neuen Daten (Labels) können Experimente durchgeführt werden 


\subsection{Visualisierung}

- Kibana 

\section{Sicherstellung der korrekten Prozesse}
Ein weiterer wichtiger Aspekt guter Datenqualität besteht darin die Prozesse der Extraktionen so zu gestalten, dass diese fehlerfrei sind.

Vollständigkeit auf Datensatzebene
Vollständigkeit auf Attributwerte (es kommen keine null-values hinzu)


Aktualität die Daten werden schnell genug abgeholt
Richtigkeit die Daten werden so abgeholt, dass sie fehlerfrei sind

Ideen:
- Source und Target vergleichen
- historisch vergleichen, wie viel zu erwarten ist
- 

Die Daten müssen innerhalb einer vorgelegten Range liegen, damit sichergestellt wird, dass die Daten in dem Zielsystem richtig ankommen.



Zuverlässigkeit, Protokollierung, Dokumentation, Audit der Prozesse
 Verfügbarkeit, Wartbarkeit, Nachvollziehbarkeit







You can also include listings from a file directly:

\lstinputlisting[language=Python,caption={This is an example of included listing},captionpos=b]{listings/example.py}
