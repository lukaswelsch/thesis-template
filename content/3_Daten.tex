\chapter{Daten}\label{ch:data}
Die in der Arbeit verwendeten Daten stammen aus dem Data Warehouse einer deutschen Bank.
Zunächst wird mit den Entwicklungsdaten gearbeitet, da diese im wesentlichen den produktiven Daten entsprechen.
In den Entwicklungsdaten wurden personenbezogene Daten, aufgrund des Datenschutzes, bereits pseudonymisiert und anonymisiert.
Des weiteren handelt es sich um die Daten der Privatkunden und nicht von Unternehmen, da der Finanzdienstleister von dem die Daten stammen sich hauptsächlich auf Kredite für Privatkunden spezialisiert hat. 


Ein- und Ausschlusskriterien für die Daten:\\
In diesem Projekt werden zwei verschiedene Datensätze verwendet. 
Zum einen wird ein Datensatz benötigt, der für die Visualisierung verwendet werden kann. 
Dieser Datensatz benötigt Eigenschaften, die sich so visualisieren lassen, dass anhand der Visualisierung neue Erkenntnisse abgeleitet werden können.
Am besten bieten sich Daten an, die nur wenige Dimensionen haben, da sich niedrig dimensionale Daten besser in einer Grafik darstellen lassen. 
\\
Des weiteren werden Daten benötigt, die für Machine Learning geeignet sind. 
Hierfür werden Daten benutzt, die feste Labels zu bestimmten Input Parametern besitzt.
Ein Klassifikator kann anhand der Eigenschaften einer Ausprägung und dem dazugehörigem Label lernen, welche Eigenschaften zu welchem Label führen und so eine Klassifikation durchführen. 


Für die Visualisierung können Metadaten zu den Prozessen verwendet werden.
Diese beinhalten die Anzahl der verarbeiteten Datensätze pro Zeiteinheit.
\\

Nach einer Recherche und Analyse der vorhandenen Daten bieten sich die Daten zum Risikoscoring am besten an.
Diese Daten sind in einer Vielzahl vorhanden und erfüllen die gewünschten Anforderung an Machine Learning. 
Die Daten sind nachfolgend genauer beschrieben: \\
Das Label ist der Risikowert eines Geschäfts, wobei ein Geschäft in diesem Fall ein Darlehen ist.
Dieser Risikowert hat einen Wertebereich von 0E bis 4E.
Die Inputdaten bestehen aus den folgenden Eigenschaften, die zum Training des Klassifikators verwendet werden können. 


% Name (Vorname), Alter, Geschlecht, Familienstand, Anzahl der Kinder,
%Alter der Kinder, Meldeadresse(n), Wohndauer, Haushaltstyp, Bildungsstand, Beruf, Arbeitgeber, Beschäftigungsdauer, monatliches Nettoeinkommen, monatliche Ausgaben (Haushaltsrechnung), Kfz-Besitz, Eintragungen in Schuldnerverzeichnissen und Warnlisten, Insolvenz, gebotene Sicherheiten (z. B. Immobilien, Bürgen), Kontoführung und Überziehungen, Dauer der Kundenbeziehung, auffällige Einzeltransaktionen, vorherige interne
%Kredite und Erfahrungen hieraus, sowie Art und Anzahl der Kredite. 




% Ein Beispiel für Daten, die aggregiert werden 

Nach \cite{sokol2005} verwenden Auskunfteien bestimmte Daten als Grundlage des Risikoscores. 
Darunter werden Kfz-Besitz, Wohndauer, verfügbares Einkommen, Berufsdaten und Haftende nicht im Data Warehouse gespeichert. 
Somit ergibt sich folgende Liste relevanter Daten für die ML-Experimente. 

\begin{itemize}
 \item Alter (Geburtsdatum)
 \item ausgeübter Beruf
 \item Bürge vorhanden
 \item Familienstand
 \item Geschlecht
 \item Kinderanzahl
 \item Kredite in Stück 
 \item Nettokreditbetrag 
 \item Nationalität
 \item Schufaauskunft
 \item Haushaltstyp
\end{itemize}


Im Rahmen der Recherche zu Daten, die sich zur Berechnung des Risikoscores anbieten konnten weitere Daten identifiziert werden, die einen Mehrwert zur Klassifikation bringen könnten. 
Diese sind der aktuelle Rückstand der Kreditzahlung und Daten aus öffentlichen Schuldnerverzeichnissen.
In dem Kapitel Experimente werden die ML-Algorithmen einmal mit und einmal ohne diesen zusätzlichen Eigenschaften getestet, um festzustellen, ob sich somit ein besseres Ergebnis erzielen lässt. 
% Wertebereich der Daten, Anzahl der Daten


\subsection{Extraktion der Daten}
Zunächst wird für die Daten ein neues Datenbank-Schema angelegt.
Ein Schema ist ein Ordner für Tabellen innerhalb einer Datenbank.

Im ersten Schritt werden die Tabellen mit Hilfe eines 'CREATE TABLE' erstellt, die gleich mit den Quelltabellen sind. 
Diese umfassen beispielsweise die Tabellen Rating, Kundenstammdaten und Geschäftsstammdaten. 
Anschließend werden diese Tabellen befüllt, indem die Daten von einem anderem Server extrahiert werden, auf dem die Daten bereits anonymisiert und pseudonymisiert gespeichert sind. 
Diese Daten entsprechen den Originaldaten, die zum Testen neuer SQL-Skripte verwendet werden. 
Da bei diesem DataWarehouse eine DataVault Architektur eingesetzt wird, müssen zu den Satelliten, die die Daten enthalten auch die Verknüpfungstabellen, Links genannt, extrahiert werden.  

Im zweiten Schritt werden die Daten mit Hilfe eines ETL-Prozesses aus dem DataWarehouse extrahiert. 
Dieses Programm wurde im Rahmen der vorliegenden Arbeit entwickelt und ist in der Versionsverwaltung gespeichert. 
%Programmiersprache, ETL Programm
Zunächst werden die benötigten Daten aus unterschiedlichen Quelltabellen %Namen der Tabellen
geladen, hierzu zählen z.B. personenbezogene Daten oder auch die tatsächlichen Risikoscorings. 
Anschließend werden die Daten bereinigt und mit Hilfe einer Aggregatfunktion die Kredite bzw. Saldo summiert und die Anzahl in einer zusätzlichen Spalte gespeichert.
Diese MetaInformationen werden mitgespeichert, da diese einen positiven Einfluss auf die Güte des Klassifikators haben könnte. 
Dies wird in den Experimenten getestet, indem der Klassifikator mit und ohne diese Zusatzinformationen getestet wird. 
Die Daten zu den Kindern, die die Kunden haben, werden in einer extra Tabelle gespeichert.
Es wird beispielsweise nicht die Anzahl der Kinder gespeichert sondern nur der Eintrag zu jedem einzelnem Kind. 
Mit Hilfe einer Aggregation auf die ID des Kunden wird die Anzahl der Kinder berechnet und gespeichert. 



\subsection{Informationen zu den Daten / Aufbau der Daten}



\subsection{Eigenschaften der Daten}
Insgesamt wurden knapp eine Million Datensätze extrahiert. 
Da zunächst die aktuellsten Daten verwendet werden, stehen allerdings nur 150 Tausend Datensätze für die Experimente zur Verfügung. 
Einzelne Eigenschaften und ihre Wertebereich sind in der nachfolgenden Tabelle aufgeführt. 

\begin{tabular}[h]{l|l|l}
Datenwert & Anzahl unterschiedlicher Werte  & Wertebereich \\ \hline
KundenID & 150.000  &  \\ \hline
PLZ & 6500  & 1015; 99974 \\ \hline
Geburtsjahr & 90  & 1913; 2001 \\ 

\end{tabular}

In der Tabelle ist eine ungewöhnliche Postleitzahl zu lesen 1015. 
Dies kommt zustande, da in den Daten nicht nur Deutsche, sondern auch internationale Adressen abgespeichert sind. 
Des weiteren sind die Berufe nicht genderneutral abgespeichert, sondern beinhalten die Formen für männliche / weibliche Bezeichnungen, wie zum Beispiel: Auszubildende. 
Im Kapitel 4 Methoden wird hierfür die Methode des Stemmings genannt und erläutert.
Mit diesem Verfahren ist es möglich die Berufe auf einen Wortstamm zurückzuführen.


\textbf{Verteilung der Klassen/Klassendichte}
Um genauere Aussagen über die Daten treffen zu können, wird zunächst die Verteilung der Klassen berechnet und dargestellt. 
Diese stellen die Grundlage für die Entscheidung der Sampling-Verfahren, die im Kapitel 4 Methoden beschrieben sind, dar.
Mit Hilfe der in DB2 verfügbaren Analytischen Funktion 'ratio_to_report' kann nachfolgendes SQL geschrieben werden, dass die prozentuale Verteilung der Klassen darstellt. 

\begin{verbatim}
SELECT ratio_to_report(count(count(*)) over() * 100 as ANTEIL 
FROM BACHELORDATEN 
GROUP BY RATING_WERT
\end{verbatim}


Anhand der nachfolgenden Tabelle ist zu erkennen, dass die Klassen sehr ungleichmäßig verteilt sind. 
Die Klasse 0E ist mit 58 \% deutlich häufiger verbreitet, als die anderen Klassen. 
Auch ist zu erkennen, dass die logisch folgende Klasse 4C einen Anteil von 0\% enthält. 
Die schlechte Verteilung der Klassen ist damit zu erklären, dass bevorzugt Kunden einen Kredit bekommen, die auch eine gute Risikobewertung haben. 
Kunden, die eine schlechte Bewertung haben, werden häufig gar nicht in die engere Auswahl genommen und tauchen somit auch nicht als Kunden im Data Warehouse auf. 
Allerdings sind Kunden, die erst nach einiger Zeit kreditunwürdig werden bzw. ihren Zahlungen nicht Folge leisten können, im Data Warehouse weiterhin gespeichert. 

\begin{tabular}[h]{l|l}
Ratingwert & Anteil (in Prozent)   \\ \hline
0E & 58\% \\ \hline
1A & 8\% \\ \hline
1B & 8\% \\ \hline
1C & 5\% \\ \hline
1D & 4.6\% \\ \hline
1E & 4\% \\ \hline
2A & 2\% \\ \hline
2B & 2\% \\ \hline
2C & 2\% \\ \hline
2D & 1\% \\ \hline
2E & 1\% \\ \hline
3A & 0.7\% \\ \hline
3B & 0.5\% \\ \hline
3C & 0.3\% \\ \hline
3D & 0.1\% \\ \hline
3E & 0.03\% \\ \hline
4A & 0.004\% \\ \hline
4B & 0.04\% \\ \hline
4C & 0\% \\ \hline
4D & 0.4\% \\ \hline
4E & 0.02\% \\ 

\end{tabular}

% Anzahl der Datensätze
% Datum von bis?
% evtl. Anzahl der Risikowerte
% Anzahl der Distinct Values



\subsection{Daten für die Visualisierung}
Für die Visualisierung können die gleichen Daten verwendet werden, die beschrieben sind.



Des weiteren werden auch Daten verwendet, die generelle Informationen zu den Prozessen im DataWarehouse enthalten. 
Diese Daten umfassen die Anzahl der exportierten Datensätze aus den einzelnen Umgebungen.





