\chapter{Daten}\label{ch:data}
Die in der Arbeit verwendeten Daten stammen aus dem Data Warehouse einer deutschen Bank.
Zunächst wird mit den Entwicklungsdaten gearbeitet, da diese im wesentlichen den produktiven Daten entsprechen.
In den Entwicklungsdaten wurden personenbezogene Daten, aufgrund des Datenschutzes, bereits pseudonymisiert und anonymisiert.


Ein- und Ausschlusskriterien für die Daten:\\
In diesem Projekt werden zwei verschiedene Datensätze verwendet. 
Zum einen wird ein Datensatz benötigt, der für die Visualisierung verwendet werden kann. 
Dieser Datensatz benötigt Eigenschaften, die sich so visualisieren lassen, dass anhand der Visualisierung neue Erkenntnisse abgeleitet werden können.
Am besten bieten sich Daten an, die nur wenige Dimensionen haben, da sich niedrig dimensionale Daten besser in einer Grafik darstellen lassen. 
\\
Des weiteren werden Daten benötigt, die für Machine Learning geeignet sind. 
Hierfür werden Daten benutzt, die feste Labels zu bestimmten Input Parametern besitzt.
Ein Klassifikator kann anhand der Eigenschaften einer Ausprägung und dem dazugehörigem Label lernen, welche Eigenschaften zu welchem Label führen und so eine Klassifikation durchführen. 


Für die Visualisierung können Metadaten zu den Prozessen verwendet werden.
Diese beinhalten die Anzahl der verarbeiteten Datensätze pro Zeiteinheit.
\\

Nach einer Recherche und Analyse der vorhandenen Daten bieten sich die Daten zum Risikoscoring am besten an.
Diese Daten sind in einer Vielzahl vorhanden und erfüllen die gewünschten Anforderung an Machine Learning. 
Die Daten sind nachfolgend genauer beschrieben: \\
Das Label ist der Risikowert eines Geschäfts, wobei ein Geschäft in diesem Fall ein Darlehen ist.
Dieser Risikowert hat einen Wertebereich von 0E bis 4E.
Die Inputdaten bestehen aus den folgenden Eigenschaften, die zum Training des Klassifikators verwendet werden können. 


% Name (Vorname), Alter, Geschlecht, Familienstand, Anzahl der Kinder,
%Alter der Kinder, Meldeadresse(n), Wohndauer, Haushaltstyp, Bildungsstand, Beruf, Arbeitgeber, Beschäftigungsdauer, monatliches Nettoeinkommen, monatliche Ausgaben (Haushaltsrechnung), Kfz-Besitz, Eintragungen in Schuldnerverzeichnissen und Warnlisten, Insolvenz, gebotene Sicherheiten (z. B. Immobilien, Bürgen), Kontoführung und Überziehungen, Dauer der Kundenbeziehung, auffällige Einzeltransaktionen, vorherige interne
%Kredite und Erfahrungen hieraus, sowie Art und Anzahl der Kredite. 

Die Daten werden mit Hilfe eines ETL-Prozesses aus dem DataWarehouse extrahiert. 
Dieses Programm wurde im Rahmen der vorliegenden Arbeit entwickelt und ist in einer Versionsverwaltung abgelegt. 
Zunächst werden die benötigten Daten aus unterschiedlichen Quelltabellen geladen, hierzu zählen z.B. personenbezogene Daten oder auch die tatsächlichen Risikoscorings. 
Anschließend werden die Daten bereinigt und mit Hilfe einer Aggregatfunktion die Kredite, Saldo summiert und die Anzahl in einer zusätzlichen Spalte gespeichert.

% Ein Beispiel für Daten, die aggregiert werden 

Nach \cite{sokol2005} verwenden Auskunfteien bestimmte Daten als Grundlage des Risikoscores. 
Darunter werden Kfz-Besitz, Wohndauer, verfügbares Einkommen, Berufsdaten und Haftende nicht im Data Warehouse gespeichert. 
Somit ergibt sich folgende Liste relevanter Daten für die ML-Experimente. 

\begin{itemize}
 \item Alter (Geburtsdatum)
 \item ausgeübter Beruf
 \item Bürge vorhanden
 \item Familienstand
 \item Geschlecht
 \item Kinderanzahl
 \item Kredite in Stück 
 \item Nettokreditbetrag 
 \item Nationalität
 \item Schufa
 \item Haushaltstyp
\end{itemize}


Im Rahmen der Recherche zu Daten, die sich zur Berechnung des Risikoscores anbieten konnten weitere Daten als nützlich identifiziert werden. 
Diese sind der aktuelle Rückstand der Kreditzahlung und Daten aus öffentlichem Schuldnerverzeichnissen.

% Wertebereich der Daten, Anzahl der Daten




Zunächst sollte sich eine Übersicht über die Daten beschafft werden:


\begin{lstlisting}[language=SQL,caption={Überblick über die Daten},captionpos=b]
SELECT VALIDDT, COUNT(VALIDDT) 
FROM SCHEMA.bachelordaten 
GROUP BY VALID_DT 
ORDER BY VALID_DT
LIMIT 5
\end{lstlisting}
Anhand dieser Abfrage lassen sich die Anzahl der Daten zu den jeweiligen Tagen bestimmen. 
Beispielhaft wird mit einem aktuellen VALIDDT gearbeitet. 
Folgende Spalten sind in der Tabelle vorhanden:



Daten für die Visualisierung:
Zur Visualisierung werden die Daten aus den Prozessen verwendet. 



