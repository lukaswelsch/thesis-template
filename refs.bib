
@inproceedings{2007,
  title = {{{METRICS FOR MEASURING DATA QUALITY}} - {{Foundations}} for an {{Economic Data Quality Management}}:},
  shorttitle = {{{METRICS FOR MEASURING DATA QUALITY}} - {{Foundations}} for an {{Economic Data Quality Management}}},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Software}} and {{Data Technologies}}},
  year = {2007},
  pages = {87--94},
  publisher = {{SciTePress - Science and and Technology Publications}},
  address = {{Barcelona, Spain}},
  doi = {10.5220/0001325600870094},
  file = {/home/luke/Zotero/storage/WENPDC3K/2007 - METRICS FOR MEASURING DATA QUALITY - Foundations f.pdf},
  isbn = {978-989-8111-05-0 978-989-8111-06-7 978-989-8111-07-4},
  language = {en}
}

@article{aljumaili2016,
  title = {Metadata-{{Based Data Quality Assessment}}},
  author = {Aljumaili, Mustafa},
  year = {2016},
  month = may,
  volume = {46},
  pages = {28},
  doi = {10.1108/VJIKMS-11-2015-0059},
  abstract = {Purpose The purpose of this paper is to develop data quality (DQ) assessment model based on content analysis and metadata analysis. Design/methodology/approach A literature review of DQ assessment models has been conducted. A study of DQ key performances (KPIs) has been done. Finally, the proposed model has been developed and applied in a case study. Findings The results of this study shows that the metadata data have important information about DQ in a database and can be used to assess DQ to provide decision support for decision makers. Originality/value There is a lot of DQ assessment in the literature; however, metadata are not considered in these models. The model developed in this study is based on metadata in addition to the content analysis, to find a quantitative DQ assessment.},
  file = {/home/luke/Zotero/storage/Z5KQBQ87/Aljumaili - 2016 - Metadata-Based Data Quality Assessment.pdf},
  journal = {V I N E}
}

@article{batini2009,
  title = {Methodologies for Data Quality Assessment and Improvement},
  author = {Batini, Carlo and Cappiello, Cinzia and Francalanci, Chiara and Maurino, Andrea},
  year = {2009},
  month = jul,
  volume = {41},
  pages = {16:1--16:52},
  issn = {0360-0300},
  doi = {10.1145/1541880.1541883},
  abstract = {The literature provides a wide range of techniques to assess and improve the quality of data. Due to the diversity and complexity of these techniques, research has recently focused on defining methodologies that help the selection, customization, and application of data quality assessment and improvement techniques. The goal of this article is to provide a systematic and comparative description of such methodologies. Methodologies are compared along several dimensions, including the methodological phases and steps, the strategies and techniques, the data quality dimensions, the types of data, and, finally, the types of information systems addressed by each methodology. The article concludes with a summary description of each methodology.},
  journal = {ACM Computing Surveys},
  keywords = {Data quality,data quality assessment,data quality improvement,data quality measurement,information system,methodology,quality dimension},
  number = {3}
}

@article{batini2009a,
  title = {Methodologies for {{Data Quality Assessment}} and {{Improvement}}},
  author = {Batini, Carlo and Cappiello, Cinzia and Francalanci, Chiara and Maurino, Andrea},
  year = {2009},
  month = jul,
  volume = {41},
  doi = {10.1145/1541880.1541883},
  abstract = {The literature provides a wide range of techniques to assess and improve the quality of data. Due to the diversity and complexity of these techniques, research has recently focused on defining methodologies that help the selection, customization, and application of data quality assessment and improvement techniques. The goal of this article is to provide a systematic and comparative description of such methodologies. Methodologies are compared along several dimensions, including the methodological phases and steps, the strategies and techniques, the data quality dimensions, the types of data, and, finally, the types of information systems addressed by each methodology. The article concludes with a summary description of each methodology.},
  file = {/home/luke/Zotero/storage/GY7QHZVX/Batini et al. - 2009 - Methodologies for Data Quality Assessment and Impr.pdf},
  journal = {ACM Comput. Surv.}
}

@article{breuer,
  title = {{Datenqualit\"ats-Cockpit (Data Quality Result Mart) zur Analyse und Steuerung der Datenqualit\"at auf Basis eines DWH nach Data Vault 2.0}},
  author = {Breuer, Christiane},
  pages = {37},
  abstract = {PDF zur Datenqualit\"at im DWH und mit Bezug auf Prozessqualit\"at},
  file = {/home/luke/Zotero/storage/GYPCJZU7/Breuer - Datenqualitäts-Cockpit (Data Quality Result Mart) .pdf},
  language = {de}
}

@article{cagala,
  title = {Improving {{Data Quality}} and {{Closing Data Gaps}} with {{Machine Learning}}},
  author = {Cagala, Tobias},
  pages = {24},
  abstract = {The identification and correction of measurement errors often involves labour intensive case-by-case evaluations by statisticians. We show how machine learning can increase the efficiency and effectiveness of these evaluations. Our approach proceeds in two steps. In the first step, a supervised learning algorithm exploits data on decisions to flag data points as erroneous to approximate the results of the human decision making process. In the second step, the algorithm applies the first-step knowledge to predict the probability of measurement errors for newly reported data points. We show that, for data on securities holdings of German banks, the algorithm yields accurate out-of-sample predictions and increases the efficiency of data quality management. While the main focus of our analysis is on an application of machine learning to data quality management, we demonstrate that the potential of machine learning for official statistics is not limited to the prediction of measurement errors. Another important problem that machine learning can help to overcome is missing data. Using simulations, we show that out-of-sample predictions of missing values with machine learning algorithms can help to close data gaps in a wide range of datasets.},
  file = {/home/luke/Zotero/storage/TXMJEZK9/Cagala - Improving Data Quality and Closing Data Gaps with .pdf},
  language = {en}
}

@book{espinosaoliva2011,
  title = {A {{Set}} of {{Experiments}} to {{Consider Data Quality Criteria}} in {{Classification Techniques}} for {{Data Mining}}},
  author = {Espinosa Oliva, Roberto and Zubcoff, Jose and Maz{\'o}n, Jose-Norberto},
  year = {2011},
  month = jun,
  pages = {694},
  doi = {10.1007/978-3-642-21887-3_51},
  abstract = {A successful data mining process depends on the data quality of the sources in order to obtain reliable knowledge. Therefore, preprocessing data is required for dealing with data quality criteria. However, preprocessing data has been traditionally seen as a time-consuming and non-trivial task since data quality criteria have to be considered without any guide about how they affect the data mining process. To overcome this situation, in this paper, we propose to analyze the data mining techniques to know the behavior of different data quality criteria on the sources and how they affects the results of the algorithms. To this aim, we have conducted a set of experiments to assess three data quality criteria: completeness, correlation and balance of data. This work is a first step towards considering, in a systematic and structured manner, data quality criteria for supporting and guiding data miners in obtaining reliable knowledge.},
  file = {/home/luke/Zotero/storage/LUCV4B9C/Espinosa Oliva et al. - 2011 - A Set of Experiments to Consider Data Quality Crit.pdf},
  isbn = {978-3-642-21886-6}
}

@article{heinrich2012,
  title = {{Ein metrikbasierter Ansatz zur Messung der Aktualit\"at von Daten in Informationssystemen}},
  author = {Heinrich, Bernd and Klier, Mathias and G{\"o}rz, Quirin},
  year = {2012},
  month = nov,
  volume = {82},
  pages = {1193--1228},
  issn = {0044-2372, 1861-8928},
  doi = {10.1007/s11573-012-0623-7},
  file = {/home/luke/Zotero/storage/IT9MMLKV/Heinrich et al. - 2012 - Ein metrikbasierter Ansatz zur Messung der Aktuali.pdf},
  journal = {Zeitschrift f\"ur Betriebswirtschaft},
  language = {de},
  number = {11}
}

@misc{JabRefFreeReferencea,
  title = {{{JabRef}} - {{Free Reference Manager}} - {{Stay}} on Top of Your {{Literature}}},
  abstract = {JabRef is a free reference manager that helps you to discover, collect, organize and cite your scholarly literature and research in an efficient way.},
  file = {/home/luke/Zotero/storage/D2SGXK5H/www.jabref.org.html},
  howpublished = {https://www.jabref.org/},
  journal = {JabRef}
}

@article{pipino2002a,
  title = {Data Quality Assessment},
  author = {Pipino, Leo L. and Lee, Yang W. and Wang, Richard Y.},
  year = {2002},
  month = apr,
  volume = {45},
  pages = {211--218},
  issn = {0001-0782},
  doi = {10.1145/505248.506010},
  abstract = {How good is a company's data quality? Answering this question requires usable data quality metrics. Currently, most data quality measures are developed on an ad hoc basis to solve specific problems [6, 8], and fundamental principles necessary for developing usable metrics in practice are lacking. In this article, we describe principles that can help organizations develop usable data quality metrics.},
  file = {/home/luke/Zotero/storage/DQ6LBWWI/Pipino et al. - 2002 - Data quality assessment.pdf},
  journal = {Communications of the ACM},
  number = {4}
}

@book{rothmann2014,
  title = {Credit {{Scoring}} in {{\"Osterreich}}},
  author = {Rothmann, Robert and {Krieger-Lamina}, Jaro and Peissl, Walter},
  year = {2014},
  month = jul,
  abstract = {Die vorliegende Studie analysiert das Ph\"anomen des Credit Scorings von Privatpersonen aus Sicht der Technikfolgenabsch\"atzung. Neben den wesentlichen rechtlichen Rahmenbedingungen sowie einer Vorstellung der zentralen Stakeholder der Branche, erfolgt vor allem eine kritische Auseinandersetzung mit den Methoden des Scorings sowie den sich daraus ergebenden sozialen Implikationen. Den Abschluss bilden Handlungsempfehlungen zu einer sozialvertr\"aglichen Gestaltung des Technologie- und Politikfeldes. Dabei l\"asst sich grunds\"atzlich sagen, dass die Verfahren des Scorings gerade durch die Digitalisierung im Zuge der letzten Jahre hinsichtlich ihrer Eingriffsintensit\"at einen wesentlichen Wandel erfahren haben. W\"ahrend die bonit\"atsbezogene Informationssammlung \"uber Privatpersonen und die Kontrolle der VerbraucherInnen \"uber sogenannte ,,Schwarze Listen`` bereits seit den 1960er Jahren existiert, geht das Verfahren des Credit Scorings in seiner Qualit\"at \"uber derartige Negativdatenbanken hinaus. Statt einer eindimensionalen Betrachtung der Zahlungsmoral erfolgt eine multidimensionale Analyse s\"amtlicher Lebensumst\"ande einer Person. Ein Blick auf die internationale Scoringlandschaft zeigt hier bedenkliche Tendenzen. So werden die Scoring-Modelle zunehmend mit externen, mitunter auch datenschutzrechtlich sensiblen Informationen angereichert die in ihrem urspr\"unglichen Entstehungskontext nicht f\"ur die Bonit\"atsbewertung gedacht waren. Diesen Entwicklungen steht eine weitgehende Unwissenheit der VerbraucherInnen um diese Verfahren gegen\"uber \textendash{} aufmerksam wird man erst, wenn negative Aspekte offenbar werden. W\"ahrend die kreditgebende Wirtschaft die Notwendigkeit des Credit Scoring als Teil des Risikomanagements begr\"undet und versucht die Vorhersage der Zahlungsausfallwahrscheinlichkeit zu pr\"azisieren, schlagen Daten- und Konsumentensch\"utzerInnen Alarm und diagnostizieren einen unverh\"altnism\"a\ss igen Eingriff in die Selbstbestimmtheit des Privatlebens der VerbraucherInnen, welche in diesem Spiel als strukturell Schw\"achere der Dominanz des Kreditsystems ausgeliefert sind. Dabei scheinen tiefgreifende Scoring-Verfahren zur Risikominimierung gerade dann unglaubw\"urdig, wenn Zahlungsausf\"alle der Schuldner ohnehin \"uber (mehrere) zus\"atzliche Sicherheiten, wie Hypotheken oder Versicherungen einkalkuliert sind. Grunds\"atzlich nachvollziehbar ist, dass die Kreditvergabe nicht ohne weitere Sicherheiten und Begutachtungsverfahren ablaufen kann, und der Gl\"aubiger den potentiellen Schuldner genauer unter die Lupe nehmen will, bevor Kredit gegeben wird. Zugleich darf jedoch nicht vergessen werden, dass Gl\"aubiger ganz wesentlich von der Kreditvergabe profitieren und diese gerade bei Banken eigentlich zur Wert- und Geldsch\"opfung dient. Die Verwendung von personenbezogenen Scoring-Verfahren zur Bonit\"atsbewertung ist ein interessenpolitisch umk\"ampftes Terrain. Dementsprechend schwierig ist es, zu konkreten Informationen bez\"uglich Mechanismen, Algorithmen und verwendeter Daten zu kommen. W\"ahrend \"uber das Scoring von Unternehmen Informationen verf\"ugbar sind, ist der Bereich des Scorings von Privatpersonen und VerbraucherInnen nahezu tabu. Von Auskunfteien, Versicherungen und der kreditgebenden Wirtschaft werden diesbez\"uglich weder die verwendeten Variablen, noch die definierten Risikoklassen offengelegt. F\"ur den einzelnen Betroffenen ist es mitunter sogar in der Hausbank nicht m\"oglich, den eigenen Score zu erfahren. W\"ahrend der konkrete Algorithmus des Scorings dem Betriebsgeheimnis unterliegt, ist die Informations- und Auskunftspflicht \"uber andere Aspekte der personenbezogenen Bewertung jedoch sogar gesetzlich festgeschrieben. Aufgrund der bestehenden Intransparenz ist jedoch ein Dialog zwischen den Vertragspartnern in der Praxis oft nicht m\"oglich. Auch die Rechtsanspr\"uche auf Richtigstellung und L\"oschung der Daten versagen weitgehend. Gerade aufgrund der Masse an Verbraucherkrediten w\"are jedoch eine verst\"arkte Problematisierung dieser Vorg\"ange angebracht. Die rasante Verbreitung neuer Informations- und Kommunikationstechnologien hat die Erfassung, Archivierung und Analyse personenbezogener Daten zudem wesentlich erleichtert und in ihrer Qualit\"at zugleich tiefer, weitreichender und subtiler gemacht. Letztlich ergeben sich daraus zahlreiche neue M\"oglichkeiten zur analytischen Vermessung von Sozialit\"at. Die digitale Spur in Form des individuellen Zahlungsverhaltens liefert einen detailreichen Einblick in das Leben von VerbraucherInnen. Ob der aktuelle Arbeitgeber, die H\"ohe der Miete, oder die letzte Onlinebestellung ganz nach dem Motto ,,Zeige mir dein Konto und ich sage dir wer du bist`` ist es der kreditgebenden Wirtschaft \"uber derartige Datensammlungen m\"oglich, ihrer KlientInnen zu analysieren und deren Glaubw\"urdigkeit und Zahlungsmoral zu werten. Mittels statistischer Prozesse werden ganze Bev\"olkerungssegmente kategorisiert und zu Gunsten der kreditgebenden Wirtschaft (aus)sortiert. Dabei entscheiden derartige Prozesse immer \"ofter dar\"uber, ob und zu welchen Konditionen VerbraucherInnen \"uberhaupt als Vertragspartner akzeptiert werden. Doch den herangezogenen Informationen und Methoden mangelt es oft an Aktualit\"at und unmittelbarem Bezug zum Zahlungsverhalten. Wie die Studie zeigt, kann die Vielschichtigkeit des Lebens durch die formale Methodik der Statistik bestenfalls ann\"ahernd, jedoch niemals vollkommen objektiv und wertfrei wiedergegeben werden. Wie bei jedem quantitativ-statistischen Verfahren, k\"onnen im Zuge von Credit-Scoring-Prozessen diverse qualitative Besonderheiten der sozialen Wirklichkeit zwangsl\"aufig nicht ad\"aquat ber\"ucksichtigt werden. So kann die automationsunterst\"utzte Kreditw\"urdigkeitsbewertung letztlich zu wirtschaftlicher Ungleichbehandlung und stereotyper Diskriminierung f\"uhren. Eine ad\"aquate Regulierung des Verbraucher-Scorings sollte daher jedenfalls die Transparenz gegen\"uber den Betroffenen sichern. Zudem ist \"uber konkretere Schranken f\"ur die Anwendung derartiger Verfahren sowie die dabei verwendeten Datenarten nachzudenken. So sollte Arbeitgebern oder Vermietern jedenfalls untersagt sein, bei ihren Entscheidungen auf Scoring- Verfahren zur\"uckzugreifen. Auch die Anwendung unterhalb einer zu bestimmenden Bagatellgrenze dient der Eingrenzung ausufernden Datensammelns und \"Uberwachens. Letztlich geht es in der Regulierung des Scorings aber auch um die praktische Durchsetzbarkeit bereits existierender Rechtsanspr\"uche.},
  file = {/home/luke/Zotero/storage/KQWPSAB9/Rothmann et al. - 2014 - Credit Scoring in Österreich.pdf},
  journal = {Institut f\"ur Technikfolgen-Absch\"atzung (ITA) der \"Osterreichischen Akademie der Wissenschaften (\"OAW); Studie in Kooperation mit der Bundesarbeiterkammer (AK) Wien; ITA-Projektbericht Nr.: A66 ISSN: 1819-1320 ISSN-online: 1818-6556}
}

@article{scannapieco2002,
  title = {Data Quality under a Computer Science Perspective},
  author = {Scannapieco, Monica and Catarci, Tiziana},
  year = {2002},
  month = jan,
  volume = {2},
  abstract = {La qualit\`a dei dati \`e una tematica affrontata in ambito statistico, gestionale, informatico, insieme a molti altri settori scientifici. Il presente articolo considera il problema della definizione della qualit\`a dei dati dal punto di vista informatico. Sono comparate alcune proposte di dimensioni (o caratteristiche) che contribuiscono alla definizione della qualit\`a dei dati e viene introdotta una definizione "base" di tale concetto. E' inoltre illustrata una classificazione che ha l'obiettivo di guidare nella scelta della definizione di qualit\`a dei dati maggiormente adeguata alle proprie esigenze.},
  file = {/home/luke/Zotero/storage/93S7Z8YY/Scannapieco und Catarci - 2002 - Data quality under a computer science perspective.pdf},
  journal = {Journal of The ACM - JACM}
}

@article{sokol2005,
  title = {{Living by numbers Leben zwischen Statistik und Wirklichkeit}},
  author = {Sokol, Bettina},
  year = {2005},
  pages = {149},
  file = {/home/luke/Zotero/storage/VBKNEECA/Sokol - Living by numbers Leben zwischen Statistik und Wir.pdf},
  language = {de}
}

@misc{sulo2006,
  title = {{{DaVis}} : {{A}} Tool for {{Visualizing Data Quality}}},
  shorttitle = {{{DaVis}}},
  author = {Sulo, R. and Eick, S. and Grossman, R.},
  year = {2006},
  abstract = {Data quality is a critical issue for the success of data-driven enterprises. The challenge for these enterprises is to provide accurate data inputs, correct codings, and accurate processing so that resulting data products are correct, accurate, and timely. Although one might think that the digitization of business and government would lead to better data, if anything, the reverse appears to be true. In our experience business data warehouses and data marts inevitably contain large amounts of poor quality data. Thus there is a need for better tools to help analysts identify and fix data quality problems. To meet this need we have created a data quality visualization tool called DaVis (Data Quality Visualizer). DaVis uses a tabular reduced visual representation to show a dataset, highlights inaccuracies and invalid data, and shows difference between versions of a dataset. Our experience in using DaVis on several consulting projects is that data quality visualization is quite useful in practice and that applying visualization techniques to address data quality problems is a fruitful research direction. CR},
  file = {/home/luke/Zotero/storage/D7NMHW74/2a6a29aee4ff97d4700f95cd4c38a727c73693dd.html},
  howpublished = {/paper/DaVis-\%3A-A-tool-for-Visualizing-Data-Quality-Sulo-Eick/2a6a29aee4ff97d4700f95cd4c38a727c73693dd},
  language = {en}
}

@article{vargas2013,
  title = {{Grunds\"atze f\"ur die effektive Aggregation von Risikodaten und die Risikoberichterstattung}},
  author = {Vargas, Fernando},
  year = {2013},
  month = jan,
  pages = {28},
  file = {/home/luke/Zotero/storage/2ZCVRIY9/Grundsätze für die effektive Aggregation von Risik.pdf},
  journal = {Basler Ausschuss f\"ur Bankenaufsicht},
  language = {de}
}

@article{wand1996,
  title = {Anchoring Data Quality Dimensions in Ontological Foundations},
  author = {Wand, Yair and Wang, Richard Y.},
  year = {1996},
  month = nov,
  volume = {39},
  pages = {86--95},
  issn = {0001-0782},
  doi = {10.1145/240455.240479},
  journal = {Communications of the ACM},
  number = {11}
}

@article{wang1996,
  title = {Beyond Accuracy: What Data Quality Means to Data Consumers},
  shorttitle = {Beyond Accuracy},
  author = {Wang, Richard Y. and Strong, Diane M.},
  year = {1996},
  month = mar,
  volume = {12},
  pages = {5--33},
  issn = {0742-1222},
  doi = {10.1080/07421222.1996.11518099},
  abstract = {Poor data quality (DQ) can have substantial social and economic impacts. Although firms are improving data quality with practical approaches and tools, their improvement efforts tend to focus narrowly on accuracy. We believe that data consumers have a much broader data quality conceptualization than IS professionals realize. The purpose of this paper is to develop a framework that captures the aspects of data quality that are important to data consumers.A two-stage survey and a two-phase sorting study were conducted to develop a hierarchical framework for organizing data quality dimensions. This framework captures dimensions of data quality that are important to data consumers. Intrinsic DQ denotes that data have quality in their own right. Contextual DQ highlights the requirement that data quality must be considered within the context of the task at hand. Representational DQ and accessibility DQ emphasize the importance of the role of systems. These findings are consistent with our understanding that high-quality data should be intrinsically good, contextually appropriate for the task, clearly represented, and accessible to the data consumer.Our framework has been used effectively in industry and government. Using this framework, IS managers were able to better understand and meet their data consumers' data quality needs. The salient feature of this research study is that quality attributes of data are collected from data consumers instead of being defined theoretically or based on researchers' experience. Although exploratory, this research provides a basis for future studies that measure data quality along the dimensions of this framework.},
  journal = {Journal of Management Information Systems},
  keywords = {data administration,data quality,database systems},
  number = {4}
}


